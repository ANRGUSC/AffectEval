{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature and model imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(\"../care_for_me\"))\n",
    "sys.path.insert(0, module_path)\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "sys.path.insert(0, module_path)\n",
    "\n",
    "import biosppy as bp\n",
    "import care_for_me\n",
    "import heartpy as hp\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyhrv\n",
    "import pyhrv.time_domain as td\n",
    "import scipy\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# from lightgbm import LightGBM\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical/common feature extraction methods\n",
    "\n",
    "WINDOW_SIZE = 60\n",
    "OVERLAP = 30\n",
    "\n",
    "def extract_mean(data, fs):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    n = data.size\n",
    "    out = []\n",
    "\n",
    "    if stop >= n:\n",
    "        segment = data\n",
    "        feature = np.mean(segment)\n",
    "        out.append(feature)\n",
    "\n",
    "    while stop < n:\n",
    "        stop = start + window_size\n",
    "        segment = data[start:stop]\n",
    "        # extract features\n",
    "        feature = np.mean(segment)\n",
    "        out.append(feature)\n",
    "        start = stop - overlap\n",
    "\n",
    "    out = list(out)\n",
    "    return out\n",
    "\n",
    "def extract_med(data, fs):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    n = data.size\n",
    "    out = []\n",
    "\n",
    "    if stop >= n:\n",
    "        segment = data\n",
    "        feature = np.median(segment)\n",
    "        out.append(feature)\n",
    "\n",
    "    while stop < n:\n",
    "        stop = start + window_size\n",
    "        segment = data[start:stop]\n",
    "        # extract features\n",
    "        feature = np.median(segment)\n",
    "        out.append(feature)\n",
    "        start = stop - overlap\n",
    "\n",
    "    out = list(out)\n",
    "    return out\n",
    "\n",
    "def extract_std(data, fs):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    n = data.size\n",
    "    out = []\n",
    "\n",
    "    if stop >= n:\n",
    "        segment = data\n",
    "        feature = np.std(segment)\n",
    "        out.append(feature)\n",
    "\n",
    "    while stop < n:\n",
    "        stop = start + window_size\n",
    "        segment = data[start:stop]\n",
    "        # extract features\n",
    "        feature = np.std(segment)\n",
    "        out.append(feature)\n",
    "        start = stop - overlap\n",
    "\n",
    "    out = list(out)\n",
    "    return out\n",
    "\n",
    "def extract_var(data, fs):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    n = data.size\n",
    "    out = []\n",
    "\n",
    "    if stop >= n:\n",
    "        segment = data\n",
    "        feature = np.var(segment)\n",
    "        out.append(feature)\n",
    "\n",
    "    while stop < n:\n",
    "        stop = start + window_size\n",
    "        segment = data[start:stop]\n",
    "        # extract features\n",
    "        feature = np.var(segment)\n",
    "        out.append(feature)\n",
    "        start = stop - overlap\n",
    "\n",
    "    out = list(out)\n",
    "    return out\n",
    "\n",
    "def extract_range(data, fs):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    n = data.size\n",
    "    out = []\n",
    "\n",
    "    if stop >= n:\n",
    "        segment = data\n",
    "        feature = np.max(segment) - np.min(segment)\n",
    "        out.append(feature)\n",
    "\n",
    "    while stop < n:\n",
    "        stop = start + window_size\n",
    "        segment = data[start:stop]\n",
    "        # extract features\n",
    "        feature = np.max(segment) - np.min(segment)\n",
    "        out.append(feature)\n",
    "        start = stop - overlap\n",
    "\n",
    "    out = list(out)\n",
    "    return out\n",
    "\n",
    "def extract_peak(data, fs):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    n = data.size\n",
    "    out = []\n",
    "\n",
    "    if stop >= n:\n",
    "        segment = data\n",
    "        feature = np.max(segment)\n",
    "        out.append(feature)\n",
    "\n",
    "    while stop < n:\n",
    "        stop = start + window_size\n",
    "        segment = data[start:stop]\n",
    "        # extract features\n",
    "        feature = np.max(segment)\n",
    "        out.append(feature)\n",
    "        start = stop - overlap\n",
    "\n",
    "    out = list(out)\n",
    "    return out\n",
    "\n",
    "def extract_slope(data, fs):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    n = data.size\n",
    "    out = []\n",
    "\n",
    "    if stop >= n:\n",
    "        segment = data\n",
    "        feature = np.gradient(segment)\n",
    "        out.append(feature)\n",
    "\n",
    "    while stop < n:\n",
    "        stop = start + window_size\n",
    "        segment = data[start:stop]\n",
    "        # extract features\n",
    "        feature = np.gradient(segment)\n",
    "        out.append(feature)\n",
    "        start = stop - overlap\n",
    "\n",
    "    out = list(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract power from frequency bands\n",
    "\n",
    "def bandpower(x, fs, fmin, fmax):\n",
    "    f, Pxx = scipy.signal.periodogram(x, fs=fs)\n",
    "    ind_min = np.argmax(f > fmin) - 1\n",
    "    ind_max = np.argmax(f > fmax) - 1\n",
    "    return np.trapezoid(Pxx[ind_min: ind_max], f[ind_min: ind_max])\n",
    "\n",
    "def extract_freq_power(data, fs, low, high):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    n = data.size\n",
    "    out = []\n",
    "\n",
    "    if stop >= n:\n",
    "        segment = data\n",
    "        # extract features\n",
    "        feature = bandpower(segment, fs, low, high)\n",
    "        out.append(feature)\n",
    "\n",
    "    while stop < n:\n",
    "        stop = start + window_size\n",
    "        segment = data[start:stop]\n",
    "        # extract features\n",
    "        feature = bandpower(segment, fs, low, high)\n",
    "        out.append(feature)\n",
    "        start = stop - overlap\n",
    "\n",
    "    out = list(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG feature extraction methods\n",
    "\n",
    "def extract_ecg_features_pyhrv(data, fs):\n",
    "    n = data.size\n",
    "    if n == 0:\n",
    "        print(\"ECG signal has length 0, returning None\")\n",
    "        return None\n",
    "    \n",
    "    hr = []\n",
    "    rmssd = []\n",
    "    sdnn = []\n",
    "\n",
    "    start = 0\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "    stop = start + window_size\n",
    "    if stop >= n:\n",
    "        t, filtered_signal, rpeaks, _, _, _, bpm = bp.signals.ecg.ecg(signal=data, sampling_rate=fs, show=False)\n",
    "        bpm = np.mean(bpm)\n",
    "        rmssd_segment = td.rmssd(rpeaks=t[rpeaks])[\"rmssd\"]\n",
    "        sdnn_segment = td.sdnn(rpeaks=t[rpeaks])[\"sdnn\"]\n",
    "\n",
    "        hr.append(bpm)\n",
    "        rmssd.append(rmssd_segment)\n",
    "        sdnn.append(sdnn_segment)\n",
    "    else:\n",
    "        while stop < n:\n",
    "            stop = start + window_size\n",
    "            segment = data[start:stop]\n",
    "            segment, info = nk.ecg_process(segment, sampling_rate=fs)\n",
    "            segment = segment[\"ECG_Clean\"]\n",
    "            t, filtered_signal, rpeaks, _, _, _, bpm = bp.signals.ecg.ecg(signal=segment, sampling_rate=fs, show=False)\n",
    "            try:\n",
    "                segment = data.iloc[start:stop]\n",
    "            except AttributeError:\n",
    "                segment = data[start:stop]\n",
    "            try:\n",
    "                bpm = np.mean(bpm)\n",
    "                rmssd_segment = td.rmssd(rpeaks=t[rpeaks])[\"rmssd\"]\n",
    "                sdnn_segment = td.sdnn(rpeaks=t[rpeaks])[\"sdnn\"]\n",
    "            except Exception as e:\n",
    "                bpm = np.nan\n",
    "                rmssd_segment = np.nan\n",
    "                sdnn_segment = np.nan\n",
    "            hr.append(bpm)\n",
    "            rmssd.append(rmssd_segment)\n",
    "            sdnn.append(sdnn_segment)\n",
    "            start = stop - overlap\n",
    "    return hr, rmssd, sdnn\n",
    "    \n",
    "def extract_hr(data, fs):\n",
    "    hr, _, _ = extract_ecg_features_pyhrv(data, fs)\n",
    "    return hr\n",
    "\n",
    "def extract_rmssd(data, fs):\n",
    "    _, rmssd, _ = extract_ecg_features_pyhrv(data, fs)\n",
    "    return rmssd\n",
    "\n",
    "def extract_sdnn(data, fs):\n",
    "    _, _, sdnn = extract_ecg_features_pyhrv(data, fs)\n",
    "    return sdnn\n",
    "\n",
    "def extract_hr_mean(data, fs):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    n = data.size\n",
    "    out = []\n",
    "\n",
    "    if stop >= n:\n",
    "        t, filtered_signal, rpeaks, _, _, _, bpm = bp.signals.ecg.ecg(signal=data, sampling_rate=fs, show=False)\n",
    "        bpm = np.mean(bpm)\n",
    "        out.append(bpm)\n",
    "    else:\n",
    "        while stop < n:\n",
    "            stop = start + window_size\n",
    "            segment = data[start:stop]\n",
    "            segment, info = nk.ecg_process(segment, sampling_rate=fs)\n",
    "            segment = segment[\"ECG_Clean\"]\n",
    "            t, filtered_signal, rpeaks, _, _, _, bpm = bp.signals.ecg.ecg(signal=segment, sampling_rate=fs, show=False)\n",
    "            try:\n",
    "                segment = data.iloc[start:stop]\n",
    "            except AttributeError:\n",
    "                segment = data[start:stop]\n",
    "            try:\n",
    "                bpm = np.mean(bpm)\n",
    "            except Exception as e:\n",
    "                bpm = np.nan\n",
    "            out.append(bpm)\n",
    "            start = stop - overlap\n",
    "    out = [np.mean(out)]\n",
    "    return out\n",
    "\n",
    "def extract_tinn(data, fs):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    n = data.size\n",
    "    out = []\n",
    "\n",
    "    if stop >= n:\n",
    "        segment = data\n",
    "        # extract features\n",
    "        feature = pyhrv.hrv.hrv(\n",
    "            signal=segment, sampling_rate=fs, plot_ecg=False, plot_Tachogram=False, show=False\n",
    "        )[\"tinn\"]\n",
    "        out.append(feature)\n",
    "\n",
    "    while stop < n:\n",
    "        stop = start + window_size\n",
    "        segment = data[start:stop]\n",
    "        # extract features\n",
    "        feature = pyhrv.hrv.hrv(\n",
    "            signal=segment, sampling_rate=fs, plot_ecg=False, plot_Tachogram=False, show=False\n",
    "        )[\"tinn\"]\n",
    "        out.append(feature)\n",
    "        start = stop - overlap\n",
    "\n",
    "    out = list(out)\n",
    "    return out\n",
    "\n",
    "def extract_nn50(data, fs):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    n = data.size\n",
    "    out = []\n",
    "\n",
    "    if stop >= n:\n",
    "        segment = data\n",
    "        # extract features\n",
    "        feature = pyhrv.hrv.hrv(\n",
    "            signal=segment, sampling_rate=fs, plot_ecg=False, plot_Tachogram=False, show=False\n",
    "        )[\"nn50\"]\n",
    "        out.append(feature)\n",
    "\n",
    "    while stop < n:\n",
    "        stop = start + window_size\n",
    "        segment = data[start:stop]\n",
    "        # extract features\n",
    "        feature = pyhrv.hrv.hrv(\n",
    "            signal=segment, sampling_rate=fs, plot_ecg=False, plot_Tachogram=False, show=False\n",
    "        )[\"nn50\"]\n",
    "        out.append(feature)\n",
    "        start = stop - overlap\n",
    "\n",
    "    out = list(out)\n",
    "    return out\n",
    "\n",
    "def extract_ulf(data, fs):\n",
    "    low = 0\n",
    "    high = 0.03\n",
    "    return extract_freq_power(data, fs, low, high)\n",
    "\n",
    "def extract_lf(data, fs):\n",
    "    low = 0.03\n",
    "    high = 0.5\n",
    "    return extract_freq_power(data, fs, low, high)\n",
    "\n",
    "def extract_hf(data, fs):\n",
    "    low = 0.12\n",
    "    high = 0.488\n",
    "    return extract_freq_power(data, fs, low, high)\n",
    "\n",
    "def extract_uhf(data, fs):\n",
    "    low = 150\n",
    "    high = 250\n",
    "    return extract_freq_power(data, fs, low, high)\n",
    "\n",
    "def extract_lf_hf_ratio(data, fs):\n",
    "    lf = extract_lf(data, fs)\n",
    "    hf = extract_hf(data, fs)\n",
    "    return np.divide(lf, hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA feature extraction\n",
    "\n",
    "# Minimum threshold by which to exclude SCRs (peaks) as relative to the largest amplitude in the signal (from neurokit documentation)\n",
    "MIN_AMP = 0.3 \n",
    "def extract_eda_features_nk(signal, fs):\n",
    "    signal = signal.astype(np.double)\n",
    "    signal = hp.scale_data(signal)\n",
    "    signal = scipy.ndimage.median_filter(signal, int(fs))\n",
    "    signals, info = nk.eda_process(signal, sampling_rate=fs)\n",
    "    phasic = signals[\"EDA_Phasic\"].to_numpy()\n",
    "    tonic = signals[\"EDA_Tonic\"].to_numpy()\n",
    "\n",
    "    peak_info = nk.eda_findpeaks(phasic, fs, amplitude_min=MIN_AMP)\n",
    "    peak_idx = peak_info[\"SCR_Peaks\"].astype(int)\n",
    "    peak_amps = peak_info[\"SCR_Height\"]\n",
    "    peaks = np.zeros(phasic.shape)\n",
    "    np.put(peaks, peak_idx, [1])\n",
    "    tonic = tonic - peaks\n",
    "\n",
    "    return tonic, peaks\n",
    "\n",
    "def extract_mean_scl(data, fs):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    out = []\n",
    "\n",
    "    tonic, _ = extract_eda_features_nk(data, fs)\n",
    "\n",
    "    if tonic is None:\n",
    "        print(\"mean SCL is None\")\n",
    "        return None\n",
    "    \n",
    "    n = tonic.size\n",
    "\n",
    "    if stop >= n:\n",
    "        segment = tonic\n",
    "        segment_mean = np.mean(segment)\n",
    "        out.append(segment_mean)\n",
    "    while stop < n:\n",
    "        stop = start + window_size\n",
    "        segment = tonic[start:stop]\n",
    "        segment_mean = np.mean(segment)\n",
    "        out.append(segment_mean)\n",
    "        start = stop - overlap\n",
    "    mean_scl = list(out)\n",
    "    return mean_scl\n",
    "\n",
    "def extract_scr_rate(data, fs):\n",
    "    window_size = int(WINDOW_SIZE*fs)\n",
    "    overlap = int(OVERLAP*fs)\n",
    "\n",
    "    start = 0\n",
    "    stop = start + window_size\n",
    "    out = []\n",
    "\n",
    "    _, peaks = extract_eda_features_nk(data, fs)\n",
    "\n",
    "    if peaks is None:\n",
    "        print(\"SCR rate is None\")\n",
    "        return None\n",
    "\n",
    "    n = peaks.size\n",
    "    \n",
    "    if stop >= n:\n",
    "        segment = peaks\n",
    "        num_peaks = sum(segment)\n",
    "        out.append(num_peaks)\n",
    "    while stop < n:\n",
    "        stop = start + window_size\n",
    "        segment = peaks[start:stop]\n",
    "        num_peaks = sum(segment)\n",
    "        out.append(num_peaks)\n",
    "        start = stop - overlap\n",
    "    scr_rate = list(out)\n",
    "    return scr_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "from care_for_me.signals import Signals, Features\n",
    "\n",
    "\n",
    "signal_types = [\n",
    "    Signals.ECG,\n",
    "    Signals.EDA\n",
    "]\n",
    "feature_names = [\n",
    "    Features.ECG_MEAN, Features.ECG_MEDIAN, Features.ECG_STD, Features.ECG_VAR,\n",
    "    Features.HR, Features.RMSSD, Features.SDNN,\n",
    "    Features.ULF, Features.LF, Features.HF, Features.LF_HF,\n",
    "    Features.EDA_MEAN, Features.EDA_MEDIAN, Features.EDA_STD, Features.EDA_VAR,\n",
    "    Features.MEAN_SCL, Features.SCR_RATE\n",
    "]\n",
    "\n",
    "# Uses default preprocessing methods\n",
    "\n",
    "feature_extraction_methods = {\n",
    "    \"ECG\": {\n",
    "        Features.ECG_MEAN: extract_mean,\n",
    "        Features.ECG_MEDIAN: extract_med,\n",
    "        Features.ECG_STD: extract_std,\n",
    "        Features.ECG_VAR: extract_var,\n",
    "        Features.HR: extract_hr, Features.RMSSD: extract_rmssd, Features.SDNN: extract_sdnn,\n",
    "        Features.LF: extract_lf, Features.HF: extract_hf, Features.LF_HF: extract_lf_hf_ratio,\n",
    "    },\n",
    "    \"EDA\": {\n",
    "        Features.EDA_MEAN: extract_mean,\n",
    "        Features.EDA_MEDIAN: extract_med,\n",
    "        Features.EDA_STD: extract_std,\n",
    "        Features.EDA_VAR: extract_var,\n",
    "        Features.MEAN_SCL: extract_mean_scl, Features.SCR_RATE: extract_scr_rate\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "The following subjects did not complete the speech exposure phase and were removed:\n",
    "- 57\n",
    "- 93\n",
    "- 16\n",
    "- 87\n",
    "- 8\n",
    "- 21\n",
    "- 88\n",
    "- 84\n",
    "- 23\n",
    "\n",
    "The following subjects did not complete the bug exposure task and were removed: \n",
    "- 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(\"../care_for_me\"))\n",
    "sys.path.insert(0, module_path)\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "sys.path.insert(0, module_path)\n",
    "\n",
    "import apd\n",
    "\n",
    "ROOT_DIR = \"/Users/emilyzhou/Desktop/Research/CAREForMe/\"\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "APD_PATH = os.path.join(DATA_DIR, \"APD\")\n",
    "SOURCE_FOLDER = os.path.join(APD_PATH, \"formatted\")\n",
    "METRICS = os.path.join(DATA_DIR, \"metrics\", \"APD\")\n",
    "\n",
    "ALL = \"all\"\n",
    "HA = \"high_anxiety_group\"\n",
    "LA = \"low_anxiety_group\"\n",
    "\n",
    "ha_participant_indices = [\n",
    "    '4', '6', '7', '8', '10', '12', '15', '16', '18', '22', '26', '27', '29', '31', '32', '33', '35', '42', '45', '47', '48', '49', '54', '55', '66', '69'\n",
    "]\n",
    "\n",
    "la_participant_indices = [\n",
    "    '14', '21', '23', '25', '34', '39', '43', '46', '51', '57', '71', '72', '77', '78', '79', '80', '82', '83', '84', '85', '87', '88', '89', '91', '92', '93'\n",
    "]\n",
    "\n",
    "SUBJECTS = ha_participant_indices.extend(la_participant_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"Grove sensor reading\" to \"EDA\"\n",
    "def get_data_files(source_folder, signal_types):\n",
    "        files_dict = {}\n",
    "        dir_list = [os.path.join(source_folder, f) for f in os.listdir(source_folder)]  # Lists all files and subdirectories\n",
    "        for p in dir_list:\n",
    "            if os.path.isdir(p):\n",
    "                files_p = os.listdir(p)\n",
    "                s = files_p[0].split(\"_\")[0]  # Get subject index from file\n",
    "                files_p = [os.path.join(p, f) for f in files_p if any(signal in f for signal in signal_types)]\n",
    "                files_dict[s] = files_p  # Add list of all files in subdirectory p\n",
    "            else:\n",
    "                print(f\"Path {p} corresponds to a file, expecting a subdirectory.\")\n",
    "        return files_dict\n",
    "\n",
    "data_files = get_data_files(SOURCE_FOLDER, [\"EDA\"])\n",
    "for subject in data_files.keys():\n",
    "    files = data_files[subject]\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        df = df.rename(columns={\"Grove sensor reading\": \"EDA\"})\n",
    "        df.to_csv(file, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate APD labels\n",
    "labels = apd.get_suds_labels(APD_PATH)\n",
    "\n",
    "def generate_labels(data):\n",
    "    \"\"\"\n",
    "    Generate binary labels for APD based on the SUDS questionnaire and the input data format.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "    :param data: Features to generate labels for. Must include subject ID and phase columns.\n",
    "    :type data: pd.DataFrame\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "    Generated labels and the unmodified input data.\n",
    "    \"\"\"\n",
    "    annotations = apd.get_suds_labels(APD_PATH)\n",
    "    labels = []\n",
    "    for i in range(data.shape[0]):\n",
    "        subject = int(data[\"subject\"].iloc[i])\n",
    "        phase = data[\"Phase\"].iloc[i]\n",
    "        label_row = annotations.loc[(annotations[\"subject\"] == subject)]\n",
    "        label = label_row[phase]\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels).ravel()\n",
    "    return labels, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running node Signal Acquisition...\n",
      "- Elapsed time: 0.0 s\n",
      "Running node Signal Preprocessor...\n",
      "- Elapsed time: 13.96 s\n",
      "Running node Feature Extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/42 [00:36<24:40, 36.10s/it]"
     ]
    }
   ],
   "source": [
    "from care_for_me.signal_acquisition.signal_acquisition import SignalAcquisition\n",
    "from care_for_me.signal_preprocessor.signal_preprocessor import SignalPreprocessor\n",
    "from care_for_me.feature_extractor.feature_extractor import FeatureExtractor\n",
    "from care_for_me.label_generator.label_generator import LabelGenerator\n",
    "from care_for_me.classification.estimator import Estimator\n",
    "from care_for_me.pipeline.pipeline import Pipeline\n",
    "\n",
    "\n",
    "label_gen = generate_labels\n",
    "signal_acq = SignalAcquisition(signal_types=signal_types, source_folder=SOURCE_FOLDER)\n",
    "signal_preprocessor = SignalPreprocessor(skip=True, resample_rate=250)\n",
    "feature_extractor = FeatureExtractor(feature_extraction_methods=feature_extraction_methods, calculate_mean=False)\n",
    "label_generator = LabelGenerator(label_generation_method=label_gen)\n",
    "\n",
    "models = {\n",
    "    \"SVM\": SVC(),\n",
    "    \"RF\": RandomForestClassifier()\n",
    "}\n",
    "\n",
    "accs = {\n",
    "    \"SVM\": [],\n",
    "    \"RF\": []\n",
    "}\n",
    "\n",
    "aucs = {\n",
    "    \"SVM\": [],\n",
    "    \"RF\": []\n",
    "}\n",
    "\n",
    "true = {\n",
    "    \"SVM\": [],\n",
    "    \"RF\": []\n",
    "}\n",
    "\n",
    "preds = {\n",
    "    \"SVM\": [],\n",
    "    \"RF\": []\n",
    "}\n",
    "\n",
    "estimator_train_val_test = Estimator(2, models, name=\"Classification: train-val-test\", random_seed=36)\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "pipeline.generate_nodes_from_layers(\n",
    "    [signal_acq, signal_preprocessor, feature_extractor, label_generator, estimator_train_val_test]\n",
    ")\n",
    "\n",
    "# We leave it up to the user to handle the final output of the pipeline. \n",
    "out = pipeline.run()\n",
    "\n",
    "# Results\n",
    "# fitted_models = out[0]\n",
    "y_true = out[1]\n",
    "y_preds = out[2]\n",
    "\n",
    "\n",
    "for model_name in models.keys():\n",
    "    model = models[model_name]\n",
    "    acc = accuracy_score(y_true, y_preds[model_name])\n",
    "    auc = roc_auc_score(y_true, y_preds[model_name])\n",
    "\n",
    "    true[model_name].append(y_true)\n",
    "    preds[model_name].append(y_preds[model_name])\n",
    "    accs[model_name].append(acc)\n",
    "    aucs[model_name].append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble, average voting:\n",
    "# ensemble_preds = np.mean(list(preds.keys()), axis=0)\n",
    "# ensemble_acc = accuracy_score(true.values()[0], ensemble_preds)\n",
    "# ensemble_auc = roc_auc_score(true.values()[0], ensemble_preds)\n",
    "\n",
    "for model_name in models.keys():\n",
    "    print(f\"{model_name} \" + \"-\"*50)\n",
    "    print(f\"\\nMean accuracy: {np.mean(accs[model_name])}\")\n",
    "    print(f\"STD accuracy: {np.std(accs[model_name])}\")\n",
    "    print(f\"Mean AUC score: {np.mean(aucs[model_name])}\")\n",
    "    print(f\"STD AUC score: {np.std(aucs[model_name])}\")\n",
    "\n",
    "print(\"Ensemble \" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WESAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(\"../care_for_me\"))\n",
    "sys.path.insert(0, module_path)\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "sys.path.insert(0, module_path)\n",
    "\n",
    "import wesad\n",
    "\n",
    "subject_indices = list(range(2, 12)) + list(range(13, 18))\n",
    "SUBJECTS = [str(i) for i in subject_indices]\n",
    "\n",
    "# NOTE: Change ROOT_DIR according to your own file structure. This will be the only place you will need to do this.\n",
    "ROOT_DIR = \"/Users/emilyzhou/Desktop/Research/CAREForMe/\"\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "WESAD_PATH = os.path.join(DATA_DIR, \"WESAD\")\n",
    "SOURCE_FOLDER = os.path.join(WESAD_PATH, \"formatted\")\n",
    "ANNOTATIONS_PATH = os.path.join(WESAD_PATH, \"annotations\")\n",
    "METRICS = os.path.join(DATA_DIR, \"metrics\", \"WESAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate WESAD labels\n",
    "labels = wesad.generate_labels(ANNOTATIONS_PATH, threshold=\"dynamic\")\n",
    "\n",
    "def generate_labels(data):\n",
    "    \"\"\"\n",
    "    Generate binary labels for WESAD based on the STAI questionnaire and the input data format.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "    :param data: Features to generate labels for. Must include subject ID and phase columns.\n",
    "    :type data: pd.DataFrame\n",
    "\n",
    "    Returns\n",
    "    --------------------\n",
    "    Generated labels and the unmodified input data.\n",
    "    \"\"\"\n",
    "    annotations = wesad.generate_labels(ANNOTATIONS_PATH, threshold=\"dynamic\")\n",
    "    labels = []\n",
    "    for i in range(data.shape[0]):\n",
    "        subject = int(data[\"subject\"].iloc[i])\n",
    "        phase = data[\"Phase\"].iloc[i]\n",
    "        label_row = annotations.loc[(annotations[\"subject\"] == subject)]\n",
    "        label = label_row[phase]\n",
    "        labels.append(label)\n",
    "    labels = np.array(labels).ravel()\n",
    "    return labels, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running node Signal Acquisition...\n",
      "- Elapsed time: 0.0 s\n",
      "Running node Signal Preprocessor...\n",
      "- Elapsed time: 0.403 s\n",
      "Running node Feature Extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Elapsed time: 2.431 s\n",
      "Running node Label Generator...\n",
      "- Elapsed time: 0.04 s\n",
      "Running node Classification: train-val-test...\n",
      "Cross-validation scores: [0.66666667 0.58333333 0.58333333 0.58333333 0.58333333]\n",
      "- Elapsed time: 0.008 s\n",
      "Running node Signal Acquisition...\n",
      "- Elapsed time: 0.0 s\n",
      "Running node Signal Preprocessor...\n",
      "- Elapsed time: 0.37 s\n",
      "Running node Feature Extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Elapsed time: 2.419 s\n",
      "Running node Label Generator...\n",
      "- Elapsed time: 0.037 s\n",
      "Running node Classification: train-val-test...\n",
      "Cross-validation scores: [0.91666667 0.95833333 0.83333333 0.70833333 0.91666667]\n",
      "- Elapsed time: 0.009 s\n",
      "Running node Signal Acquisition...\n",
      "- Elapsed time: 0.0 s\n",
      "Running node Signal Preprocessor...\n",
      "- Elapsed time: 0.358 s\n",
      "Running node Feature Extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Elapsed time: 2.494 s\n",
      "Running node Label Generator...\n",
      "- Elapsed time: 0.045 s\n",
      "Running node Classification: train-val-test...\n",
      "Cross-validation scores: [0.94444444 0.91666667 0.91666667 0.94444444 0.97222222]\n",
      "- Elapsed time: 0.01 s\n",
      "Running node Signal Acquisition...\n",
      "- Elapsed time: 0.0 s\n",
      "Running node Signal Preprocessor...\n",
      "- Elapsed time: 0.364 s\n",
      "Running node Feature Extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Elapsed time: 2.424 s\n",
      "Running node Label Generator...\n",
      "- Elapsed time: 0.046 s\n",
      "Running node Classification: train-val-test...\n",
      "Cross-validation scores: [1. 1. 1. 1. 1.]\n",
      "- Elapsed time: 0.01 s\n",
      "Running node Signal Acquisition...\n",
      "- Elapsed time: 0.0 s\n",
      "Running node Signal Preprocessor...\n",
      "- Elapsed time: 0.344 s\n",
      "Running node Feature Extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Elapsed time: 2.535 s\n",
      "Running node Label Generator...\n",
      "- Elapsed time: 0.056 s\n",
      "Running node Classification: train-val-test...\n",
      "Cross-validation scores: [1.   1.   1.   0.95 1.  ]\n",
      "- Elapsed time: 0.012 s\n",
      "Running node Signal Acquisition...\n",
      "- Elapsed time: 0.0 s\n",
      "Running node Signal Preprocessor...\n",
      "- Elapsed time: 0.4 s\n",
      "Running node Feature Extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Elapsed time: 2.326 s\n",
      "Running node Label Generator...\n",
      "- Elapsed time: 0.058 s\n",
      "Running node Classification: train-val-test...\n",
      "Cross-validation scores: [1.         0.95833333 1.         1.         1.        ]\n",
      "- Elapsed time: 0.1 s\n",
      "Running node Signal Acquisition...\n",
      "- Elapsed time: 0.0 s\n",
      "Running node Signal Preprocessor...\n",
      "- Elapsed time: 0.347 s\n",
      "Running node Feature Extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Elapsed time: 2.31 s\n",
      "Running node Label Generator...\n",
      "- Elapsed time: 0.11 s\n",
      "Running node Classification: train-val-test...\n",
      "Cross-validation scores: [1.         0.95238095 1.         1.         1.        ]\n",
      "- Elapsed time: 0.102 s\n",
      "Running node Signal Acquisition...\n",
      "- Elapsed time: 0.0 s\n",
      "Running node Signal Preprocessor...\n",
      "- Elapsed time: 0.345 s\n",
      "Running node Feature Extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Elapsed time: 2.294 s\n",
      "Running node Label Generator...\n",
      "- Elapsed time: 0.068 s\n",
      "Running node Classification: train-val-test...\n",
      "Cross-validation scores: [1. 1. 1. 1. 1.]\n",
      "- Elapsed time: 0.103 s\n",
      "Running node Signal Acquisition...\n",
      "- Elapsed time: 0.0 s\n",
      "Running node Signal Preprocessor...\n",
      "- Elapsed time: 0.343 s\n",
      "Running node Feature Extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Elapsed time: 2.303 s\n",
      "Running node Label Generator...\n",
      "- Elapsed time: 0.075 s\n",
      "Running node Classification: train-val-test...\n",
      "Cross-validation scores: [1. 1. 1. 1. 1.]\n",
      "- Elapsed time: 0.105 s\n",
      "Running node Signal Acquisition...\n",
      "- Elapsed time: 0.0 s\n",
      "Running node Signal Preprocessor...\n",
      "- Elapsed time: 0.345 s\n",
      "Running node Feature Extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Elapsed time: 2.362 s\n",
      "Running node Label Generator...\n",
      "- Elapsed time: 0.085 s\n",
      "Running node Classification: train-val-test...\n",
      "Cross-validation scores: [1. 1. 1. 1. 1.]\n",
      "- Elapsed time: 0.108 s\n"
     ]
    }
   ],
   "source": [
    "from care_for_me.signal_acquisition.signal_acquisition import SignalAcquisition\n",
    "from care_for_me.signal_preprocessor.signal_preprocessor import SignalPreprocessor\n",
    "from care_for_me.feature_extractor.feature_extractor import FeatureExtractor\n",
    "from care_for_me.label_generator.label_generator import LabelGenerator\n",
    "from care_for_me.classification.estimator import Estimator\n",
    "from care_for_me.pipeline.pipeline import Pipeline\n",
    "\n",
    "\n",
    "label_gen = generate_labels\n",
    "signal_acq = SignalAcquisition(signal_types=signal_types, source_folder=SOURCE_FOLDER)\n",
    "signal_preprocessor = SignalPreprocessor(skip=True, resample_rate=250)\n",
    "feature_extractor = FeatureExtractor(feature_extraction_methods=feature_extraction_methods, calculate_mean=True)\n",
    "label_generator = LabelGenerator(label_generation_method=label_gen)\n",
    "\n",
    "models = {\n",
    "    \"SVM\": SVC(C=1, gamma=0.01),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=30)\n",
    "}\n",
    "\n",
    "accs = {\n",
    "    \"SVM\": [],\n",
    "    \"RF\": []\n",
    "}\n",
    "\n",
    "aucs = {\n",
    "    \"SVM\": [],\n",
    "    \"RF\": []\n",
    "}\n",
    "\n",
    "true = {\n",
    "    \"SVM\": [],\n",
    "    \"RF\": []\n",
    "}\n",
    "\n",
    "preds = {\n",
    "    \"SVM\": [],\n",
    "    \"RF\": []\n",
    "}\n",
    "\n",
    "estimator_train_val_test = Estimator(2, models, name=\"Classification: train-val-test\", random_seed=36)\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "pipeline.generate_nodes_from_layers(\n",
    "    [signal_acq, signal_preprocessor, feature_extractor, label_generator, estimator_train_val_test]\n",
    ")\n",
    "\n",
    "# We leave it up to the user to handle the final output of the pipeline. \n",
    "out = pipeline.run()\n",
    "\n",
    "# Results\n",
    "# fitted_models = out[0]\n",
    "y_true = out[1]\n",
    "y_preds = out[2]\n",
    "\n",
    "\n",
    "for model_name in models.keys():\n",
    "    model = models[model_name]\n",
    "    acc = accuracy_score(y_true, y_preds[model_name])\n",
    "    auc = roc_auc_score(y_true, y_preds[model_name])\n",
    "\n",
    "    true[model_name].append(y_true)\n",
    "    preds[model_name].append(y_preds[model_name])\n",
    "    accs[model_name].append(acc)\n",
    "    aucs[model_name].append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM --------------------------------------------------\n",
      "Mean accuracy: 0.8933333333333333\n",
      "STD accuracy: 0.155492050529208\n",
      "Mean AUC score: 0.8484848484848484\n",
      "STD AUC score: 0.2289834604522735\n",
      "\n",
      "RF --------------------------------------------------\n",
      "Mean accuracy: 1.0\n",
      "STD accuracy: 0.0\n",
      "Mean AUC score: 1.0\n",
      "STD AUC score: 0.0\n",
      "\n",
      "15\n",
      "90\n",
      "30\n",
      "105\n",
      "45\n",
      "120\n",
      "60\n",
      "135\n",
      "75\n",
      "150\n",
      "Ensemble --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ensemble, average voting:\n",
    "# ensemble_preds = np.mean(list(preds.keys()), axis=0)\n",
    "# ensemble_acc = accuracy_score(true.values()[0], ensemble_preds)\n",
    "# ensemble_auc = roc_auc_score(true.values()[0], ensemble_preds)\n",
    "\n",
    "for model_name in models.keys():\n",
    "    print(f\"{model_name} \" + \"-\"*50)\n",
    "    print(f\"Mean accuracy: {np.mean(accs[model_name])}\")\n",
    "    print(f\"STD accuracy: {np.std(accs[model_name])}\")\n",
    "    print(f\"Mean AUC score: {np.mean(aucs[model_name])}\")\n",
    "    print(f\"STD AUC score: {np.std(aucs[model_name])}\")\n",
    "    print()\n",
    "\n",
    "for i in range(5):\n",
    "    ensemble_preds = []\n",
    "    ensemble_true = []\n",
    "    for model_name in models.keys():\n",
    "        pred = preds[model_name][i]\n",
    "        y_true = true[model_name][i]\n",
    "        print(len(pred))\n",
    "        ensemble_preds.append(pred)\n",
    "    \n",
    "print(\"Ensemble \" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4255300\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = \"/Users/emilyzhou/Desktop/Research/CAREForMe/\"\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "WESAD_PATH = os.path.join(DATA_DIR, \"WESAD\")\n",
    "SOURCE_FOLDER = os.path.join(WESAD_PATH, \"original\")\n",
    "\n",
    "file = os.path.join(SOURCE_FOLDER, \"S2\", \"S2.pkl\")\n",
    "data = pd.read_pickle(file)\n",
    "labels = data[\"label\"]\n",
    "phase = 1 # baseline\n",
    "# phase = 2 # stress\n",
    "# phase = 3 # amusement\n",
    "indices = [i for i, x in enumerate(labels) if x == phase]\n",
    "signal = data[\"signal\"][\"chest\"][\"ECG\"]\n",
    "phase = [signal[i] for i in indices]\n",
    "# print(indices[0:500])\n",
    "print(len(signal))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
